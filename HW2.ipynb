{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2: \n",
    "## Discovery of Frequent Itemsets and Association Rules\n",
    "### Group 11\n",
    "\n",
    "Boyu Xue (boyux@kth.se), Kristupas Bajarunas(kristupasbajarunas@gmail.com)\n",
    "\n",
    "\n",
    "### Description:\n",
    "\n",
    "The problem of discovering association rules between itemsets in a sales transaction database (a set of baskets) includes the following two sub-problems [R. Agrawal and R. Srikant, VLDB '94]:\n",
    "\n",
    "#### Finding frequent itemsets with support at least s;\n",
    "Generating association rules with confidence at least c from the itemsets found in the first step.\n",
    "Remind that an association rule is an implication X → Y, where X and Y are itemsets such that X∩Y=∅. Support of the rule X → Y is the number of transactions that contain X⋃Y. Confidence of the rule X → Y the fraction of transactions containing X⋃Y in all transactions that contain X.\n",
    "\n",
    "#### Task\n",
    "You are to solve the first sub-problem: to implement the Apriori algorithm for finding frequent itemsets with support at least s in a dataset of sales transactions. Remind that support of an itemset is the number of transactions containing the itemset. To test and evaluate your implementation, write a program that uses your Apriori algorithm implementation to discover frequent itemsets with support at least s in a given dataset of sales transactions.\n",
    "\n",
    "The implementation can be done using any big data processing framework, such as Apache Spark, Apache Flink, or no framework, e.g., in Java, Python, etc.  \n",
    "\n",
    "#### Optional task for extra bonus\n",
    "\n",
    "Solve the second sub-problem, i.e., develop and implement an algorithm for generating association rules between frequent itemsets discovered by using the Apriori algorithm in a dataset of sales transactions. The rules must have support at least s and confidence at least c, where s and c are given as input parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import libiaries and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Read the file\n",
    "\n",
    "1. Split the space \" \" between the transactions from the file.\n",
    "2. Return the singletons as integers and append them to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file=\"./data/T10I4D100K.dat\"\n",
    "database = []\n",
    "for l in open(data_file, 'r'):\n",
    "    items = l.strip().split(' ')\n",
    "    singletons = list(map(int, items)) # convert items to integers\n",
    "    database.append(singletons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creat some parameters\n",
    "\n",
    "1. Support threshold of frequent itemsets:Since there are many items(100,000) in this database, the support degree of each frequent itemsets will be very low. So we use 1% as a support degree, which given a support threshould of 1,000.\n",
    "\n",
    "2. \"max_k\" is the maximum number of items in a candidate k-itemsets, we use 5 as the value of the \"max_k\".\n",
    "\n",
    "3. For \"k\" in the range of \"max_k\", we create a blank dictionary \"L\" which will contains all the frequent itemsets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "support=100000*0.01\n",
    "conf=0.5\n",
    "max_k=5\n",
    "L=[None] * max_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Define the function \"createC1\" to get the candidate singletons\n",
    "\n",
    "1. C1 is the candidate singletons sets that might be frequent sets.\n",
    "2. We use \"frozenset\" to create the keys in the dictionary of frequent sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createC1(database):\n",
    "    C1 = defaultdict(set)\n",
    "    transaction_num=-1\n",
    "    for transaction in database:\n",
    "        transaction_num+=1\n",
    "        for item in transaction:\n",
    "            C1[frozenset({item})].add(transaction_num)\n",
    "    return C1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = createC1(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Define the \"filter_remove\" function to filter and remove the non-frequent items\n",
    "\n",
    "1. Create a delete_list which include the non-frequent items according to the support threshold.\n",
    "2. Delete the non-frequent items from the candidate singleton list C1.\n",
    "3. Put the set of truly frequent singletons on dictionary L, as the first set L[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_remove(C1,support):\n",
    "    delete_list=[]\n",
    "    for key in C1.keys():\n",
    "        if len(C1[key])<support:\n",
    "            delete_list.append(key)\n",
    "    for item in delete_list:\n",
    "        del(C1[item])\n",
    "    return C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L[0]=filter_remove(C1,support)\n",
    "#L[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Filter and create the set of truly frequent k-itemsets\n",
    "1. Combine the L[0] with L[k-2]. In the first loop, we can create a new 2-itemset which include two singletons, and return the truly frequent pairs; in the second loop, we can create a triple which include one singleton and one pair, and return the truly frequent triples.\n",
    "2. For each new k-itemset, get the values of (k-1)-itemset and 1-itemset, and calculate the intersection of the two values. Then we can get the new value for the new k-itemset, e.g., if we use “frozenset({25}) :{0,2}” and “frozenset({52}) :{0,3}”  to create a new 2-itemset “frozenset({25, 52}” , the value of  “frozenset({25, 52})” will be the intersection of {0,2} and {0,3} ={0}, then we get “frozenset({25, 52}: {0}” .\n",
    "2. Print the result of frequent k_itemsets (singletons, pairs, triples).  Enumerate some of the result (no more than 50 in each k_itensets). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2, max_k+1):\n",
    "    single_prod = L[0]\n",
    "    previous_prod = L[k - 2]\n",
    "    new_prod_set = defaultdict(set)\n",
    "    for keyA in single_prod.keys():\n",
    "        for keyB in previous_prod.keys():\n",
    "            new_item_set = frozenset(keyA.union(keyB))\n",
    "            if len(new_item_set) != k:\n",
    "                continue\n",
    "            new_prod_set[new_item_set] = single_prod[keyA].intersection(previous_prod[keyB])\n",
    "    filter_remove(new_prod_set, support)\n",
    "    L[k-1] = new_prod_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frequent 1-itemsets with support 1000.0 = 375\n",
      "Items: frozenset({25}) -> Support: 1395\n",
      "Items: frozenset({52}) -> Support: 1983\n",
      "Items: frozenset({240}) -> Support: 1399\n",
      "Items: frozenset({274}) -> Support: 2628\n",
      "Items: frozenset({368}) -> Support: 7828\n",
      "Items: frozenset({448}) -> Support: 1370\n",
      "Items: frozenset({538}) -> Support: 3982\n",
      "Items: frozenset({561}) -> Support: 2783\n",
      "Items: frozenset({630}) -> Support: 1523\n",
      "Items: frozenset({687}) -> Support: 1762\n",
      "Number of frequent 2-itemsets with support 1000.0 = 9\n",
      "Items: frozenset({368, 682}) -> Support: 1193\n",
      "Items: frozenset({368, 829}) -> Support: 1194\n",
      "Items: frozenset({825, 39}) -> Support: 1187\n",
      "Items: frozenset({704, 825}) -> Support: 1102\n",
      "Items: frozenset({704, 39}) -> Support: 1107\n",
      "Items: frozenset({227, 390}) -> Support: 1049\n",
      "Items: frozenset({722, 390}) -> Support: 1042\n",
      "Items: frozenset({217, 346}) -> Support: 1336\n",
      "Items: frozenset({829, 789}) -> Support: 1194\n",
      "Number of frequent 3-itemsets with support 1000.0 = 1\n",
      "Items: frozenset({704, 825, 39}) -> Support: 1035\n",
      "Number of frequent 4-itemsets with support 1000.0 = 0\n",
      "Number of frequent 5-itemsets with support 1000.0 = 0\n"
     ]
    }
   ],
   "source": [
    "for k, k_itemsets in enumerate(L,1):\n",
    "    print(\"Number of frequent {}-itemsets with support {} = {}\".format(k, support, len(k_itemsets)))\n",
    "    for count, (itemset, transactions) in enumerate(k_itemsets.items(),1):\n",
    "        print(\"Items: {} -> Support: {}\".format(itemset, len(transactions)))\n",
    "        if count == 10:\n",
    "              break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.Generate association rules and calculate the confidence of rules\n",
    "\n",
    "1. The association rules I -> j means 2 or more than 2 items( I(i1,i2,...Ik) and j) usually been included in the same basket, so we only take the frequent 2-itemsets and 3-itemsets (L[1:]) to the following analysis.\n",
    "2. Generate new set (genset) includes x length of elements from the input frequent itemsets. For example, for {25,52}, if x=0, the new \"genset\" include 1 item from the itemsets, it will be [(25,), (52,)], if x=1, the genset will be [(25, 52)].\n",
    "3. Generate \"newset\" which is used to be the associated item \"I\" of the association rules, and \"frozen\" the newset to create the keys in a dictionary. \n",
    "4. Generate the \"denom\" as the associated item \"j\" of the association rules. The length of denom = 0 means there is no associated item \"j\".\n",
    "5. The \"numer_support\" is the support of itemsets which contain both \"I\" and \"j\", the \"newset_support\" is the support of itemsets which contain the the associated item \"I\" of the association rules. The confidence of a rule can be calculated by using numer_support/newset_support. (Note: in L[0] each itemset include one frequent item, which means the length of newset=1.)\n",
    "6. Return the association rules according to a specific confident threshold(\"conf\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules(L,conf):\n",
    "    rules=[]\n",
    "    for val in L[1:]:\n",
    "        for items, trans in val.items():\n",
    "            numer_support=len(trans)\n",
    "            for x in range(len(items)):\n",
    "                genset=list(itertools.combinations(items, x+1))\n",
    "                for sets in genset:\n",
    "                    newset=frozenset(sets)\n",
    "                    newset_support=len(L[len(newset)-1][newset])# len(pairs)=2, L[1] is about the pairs.\n",
    "                    denom=items-newset\n",
    "                    if len(denom)==0:\n",
    "                        continue\n",
    "                    confidence=numer_support/newset_support\n",
    "                    if confidence>=conf:\n",
    "                        rules.append((newset,denom,confidence))\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[704] -> [825]                         (confidence: 0.6142697881828316)\n",
      "[704] -> [39]                          (confidence: 0.617056856187291)\n",
      "[227] -> [390]                         (confidence: 0.577007700770077)\n",
      "[704] -> [825, 39]                     (confidence: 0.5769230769230769)\n",
      "[704, 825] -> [39]                          (confidence: 0.9392014519056261)\n",
      "[704, 39] -> [825]                         (confidence: 0.9349593495934959)\n",
      "[825, 39] -> [704]                         (confidence: 0.8719460825610783)\n"
     ]
    }
   ],
   "source": [
    "r = rules(L, conf)\n",
    "n_rules = len(r)\n",
    "for i in range(n_rules):\n",
    "    m1 = list(r[i][0])[0:]\n",
    "    m2=list(r[i][1])[0:]\n",
    "    m3=r[i][2]\n",
    "    print(\"{} -> {!s:<30}(confidence: {})\".format(m1,m2,m3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
